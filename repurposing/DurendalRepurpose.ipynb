{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, GRUCell\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "\n",
    "import random\n",
    "\n",
    "import bisect\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, structured_negative_sampling\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.transforms import SVDFeatureReduction\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.transforms import RandomLinkSplit,NormalizeFeatures,Constant,OneHotDegree\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv,SAGEConv,GATv2Conv, GINConv, Linear, GCN, GAT\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "sys.path.insert(1, '../steemitth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bceaca5",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from durendalrepurpose import *\n",
    "from steemit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa53dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#snapshots = get_steemit_dataset(preprocess='textBERT') text features available upon request\n",
    "snapshots = get_steemit_dataset(preprocess='constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1df190",
   "metadata": {},
   "source": [
    "# TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fea0d",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_insort(a, x, lo=0, hi=None):\n",
    "    \"\"\"Insert item x in list a, and keep it reverse-sorted assuming a\n",
    "    is reverse-sorted.\n",
    "\n",
    "    If x is already in a, insert it to the right of the rightmost x.\n",
    "\n",
    "    Optional args lo (default 0) and hi (default len(a)) bound the\n",
    "    slice of a to be searched.\n",
    "    \n",
    "    Function useful to compute MRR.\n",
    "    \"\"\"\n",
    "    if lo < 0:\n",
    "        raise ValueError('lo must be non-negative')\n",
    "    if hi is None:\n",
    "        hi = len(a)\n",
    "    while lo < hi:\n",
    "        mid = (lo+hi)//2\n",
    "        if x > a[mid]: hi = mid\n",
    "        else: lo = mid+1\n",
    "    a.insert(lo, x)\n",
    "    return lo\n",
    "\n",
    "def compute_mrr(real_scores, fake_scores):\n",
    "    srr = 0\n",
    "    count = 0\n",
    "    for i,score in enumerate(real_scores):\n",
    "        try:\n",
    "            fake_scores_cp = copy.copy([fake_scores[i]])\n",
    "        except IndexError: break\n",
    "        rank = reverse_insort(fake_scores_cp, score)\n",
    "        rr = 1/(rank+1) #index starts from zero\n",
    "        srr+=rr\n",
    "        count+=1\n",
    "    return srr/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332621fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in hetdata.x_dict.items()}\n",
    "    \n",
    "    #DURENDALRGCN\n",
    "    \n",
    "    rgcn = DurendalRepurpose(in_channels, hetdata.metadata(),\n",
    "                        hidden_conv_1=hidden_conv_1,\n",
    "                        hidden_conv_2=hidden_conv_2,\n",
    "                        model='rgcn')\n",
    "    \n",
    "    rgcn.reset_parameters()\n",
    "    \n",
    "    rgcnopt = torch.optim.Adam(params=rgcn.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #DURENDALHAN\n",
    "    \n",
    "    han = DurendalRepurpose(in_channels, hetdata.metadata(),\n",
    "                        hidden_conv_1=hidden_conv_1,\n",
    "                        hidden_conv_2=hidden_conv_2,\n",
    "                        model='han')\n",
    "    \n",
    "    han.reset_parameters()\n",
    "    \n",
    "    hanopt = torch.optim.Adam(params=han.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #DURENDALHGT\n",
    "    \n",
    "    hgt = DurendalRepurpose(in_channels, hetdata.metadata(),\n",
    "                        hidden_conv_1=hidden_conv_1,\n",
    "                        hidden_conv_2=hidden_conv_2,\n",
    "                        model='hgt')\n",
    "    \n",
    "    hgt.reset_parameters()\n",
    "    \n",
    "    hgtopt = torch.optim.Adam(params=hgt.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    \n",
    "    past_dict_1_rgcn = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_1_rgcn[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_1_rgcn[src][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_1_rgcn[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[dst].num_nodes)])\n",
    "        \n",
    "    past_dict_2_rgcn = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_2_rgcn[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_2_rgcn[src][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_2_rgcn[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[dst].num_nodes)])\n",
    "    \n",
    "    past_dict_1_han = copy.deepcopy(past_dict_1_rgcn)\n",
    "    past_dict_2_han = copy.deepcopy(past_dict_2_rgcn)\n",
    "    \n",
    "    past_dict_1_hgt = copy.deepcopy(past_dict_1_rgcn)\n",
    "    past_dict_2_hgt = copy.deepcopy(past_dict_2_rgcn)\n",
    "    \n",
    "    rgcn_avgpr = 0\n",
    "    rgcn_mrr = 0\n",
    "    han_avgpr = 0\n",
    "    han_mrr = 0\n",
    "    hgt_avgpr = 0\n",
    "    hgt_mrr = 0\n",
    "    \n",
    "    for i in range(num_snap-1):\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "                #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        \n",
    "        hom_transform = RandomLinkSplit(num_val=0.0, num_test=0.20)\n",
    "        \n",
    "        \n",
    "        rel0_hom = Data()\n",
    "        rel0_hom.x = copy.deepcopy(snapshot['node'].x)\n",
    "        rel0_hom.edge_index = copy.deepcopy(snapshot['node','follow','node'].edge_index)\n",
    "        rel0_train, _, rel0_val = hom_transform(rel0_hom)\n",
    "        \n",
    "        \n",
    "        het_train_data = copy.deepcopy(snapshot)\n",
    "        het_val_data = copy.deepcopy(snapshot)\n",
    "        \n",
    "        het_train_data['node','follow','node'].edge_index = rel0_train.edge_index\n",
    "        het_train_data['node','follow','node'].edge_label_index = rel0_train.edge_label_index\n",
    "        het_train_data['node','follow','node'].edge_label = rel0_train.edge_label\n",
    "        het_val_data['node','follow','node'].edge_index = rel0_val.edge_index\n",
    "        het_val_data['node','follow','node'].edge_label_index = rel0_val.edge_label_index\n",
    "        het_val_data['node','follow','node'].edge_label = rel0_val.edge_label\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['node','follow','node'].edge_index, #positive edges\n",
    "            num_nodes=het_test_data['node'].num_nodes, # number of nodes\n",
    "            num_neg_samples=het_test_data['node','follow','node'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['node','follow','node'].edge_index.size(1)\n",
    "        het_test_data['node','follow','node'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['node','follow','node'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['node','follow','node'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['node','follow','node'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['node','follow','node'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "        het_val_data['node','follow','node'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_test_data['node','follow','node'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        rgcn, rgcn_avgpr_test, rgcn_mrr_test , past_dict_1_rgcn, past_dict_2_rgcn, rgcnopt =\\\n",
    "            durendal_train_single_snapshot(rgcn, snapshot, i, het_train_data, het_val_data, het_test_data,\\\n",
    "                                  past_dict_1_rgcn, past_dict_2_rgcn, rgcnopt)\n",
    "        \n",
    "        han, han_avgpr_test, han_mrr_test , past_dict_1_han, past_dict_2_han, hanopt =\\\n",
    "            durendal_train_single_snapshot(han, snapshot, i, het_train_data, het_val_data, het_test_data,\\\n",
    "                                  past_dict_1_han, past_dict_2_han, hanopt)\n",
    "        \n",
    "        hgt, hgt_avgpr_test, hgt_mrr_test , past_dict_1_hgt, past_dict_2_hgt, hgtopt =\\\n",
    "            durendal_train_single_snapshot(hgt, snapshot, i, het_train_data, het_val_data, het_test_data,\\\n",
    "                                  past_dict_1_hgt, past_dict_2_hgt, hgtopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {i} done\\n')\n",
    "        #print(f' DURENDAL-RGCN AVGPR Test: {rgcn_avgpr_test} \\n MRR Test: {rgcn_mrr_test}\\n')\n",
    "        #print(f' DURENDAL-HAN AVGPR Test: {han_avgpr_test} \\n MRR Test: {han_mrr_test}\\n')\n",
    "        #print(f' DURENDAL-HGT AVGPR Test: {hgt_avgpr_test} \\n MRR Test: {hgt_mrr_test}\\n')\n",
    "        rgcn_avgpr += rgcn_avgpr_test\n",
    "        rgcn_mrr += rgcn_mrr_test\n",
    "        han_avgpr += han_avgpr_test\n",
    "        han_mrr += han_mrr_test\n",
    "        hgt_avgpr += hgt_avgpr_test\n",
    "        hgt_mrr += hgt_mrr_test\n",
    "        \n",
    "        \n",
    "    rgcn_avgpr_all = rgcn_avgpr / (num_snap-1)\n",
    "    rgcn_mrr_all = rgcn_mrr / (num_snap-1)\n",
    "    han_avgpr_all = han_avgpr / (num_snap-1)\n",
    "    han_mrr_all = han_mrr / (num_snap-1)\n",
    "    hgt_avgpr_all = hgt_avgpr / (num_snap-1)\n",
    "    hgt_mrr_all = hgt_mrr / (num_snap-1)\n",
    "    \n",
    "    print('DURENDAL-RGCN')\n",
    "    print(f'\\tAVGPR over time: Test: {rgcn_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {rgcn_mrr_all}')\n",
    "    print()\n",
    "    print('DURENDAL-HAN')\n",
    "    print(f'\\tAVGPR over time: Test: {han_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {han_mrr_all}')\n",
    "    print()\n",
    "    print('DURENDAL-HGT')\n",
    "    print(f'\\tAVGPR over time: Test: {hgt_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {hgt_mrr_all}')\n",
    "    print()\n",
    "    \n",
    "    return rgcn_avgpr_all, han_avgpr_all, hgt_avgpr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b883f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def durendal_train_single_snapshot(model, data, i_snap, train_data, val_data, test_data,\\\n",
    "                          past_dict_1, past_dict_2,\\\n",
    "                          optimizer, device='cpu', num_epochs=50, verbose=False):\n",
    "    \n",
    "    mrr_val_max = 0\n",
    "    avgpr_val_max = 0\n",
    "    best_model = model\n",
    "    train_data = train_data.to(device)\n",
    "    best_epoch = -1\n",
    "    best_past_dict_1 = {}\n",
    "    best_past_dict_2 = {}\n",
    "    \n",
    "    tol = 5e-2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        ## Note\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Compute loss and backpropagate\n",
    "        ## 3. Update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        pred, past_dict_1, past_dict_2 =\\\n",
    "            model(train_data.x_dict, train_data.edge_index_dict, train_data['node','follow','node'].edge_label_index,\\\n",
    "                  i_snap, past_dict_1, past_dict_2)\n",
    "        \n",
    "        loss = model.loss(pred, train_data['node','follow','node'].edge_label.type_as(pred)) #loss to fine tune on current snapshot\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        log = 'Epoch: {:03d}\\n AVGPR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n MRR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n F1-Score Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n Loss: {}'\n",
    "        avgpr_score_val, mrr_val = durendal_test(model, i_snap, val_data, data, device)\n",
    "        \n",
    "        \"\"\"\n",
    "        if mrr_val_max-tol < mrr_val:\n",
    "            mrr_val_max = mrr_val\n",
    "            best_epoch = epoch\n",
    "            best_current_embeddings = current_embeddings\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        #print(f'Epoch: {epoch} done')\n",
    "            \n",
    "        \"\"\"\n",
    "        if avgpr_val_max-tol <= avgpr_score_val:\n",
    "            avgpr_val_max = avgpr_score_val\n",
    "            best_epoch = epoch\n",
    "            best_past_dict_1 = past_dict_1\n",
    "            best_past_dict_2 = past_dict_2\n",
    "            best_model = model\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    avgpr_score_test, mrr_test = durendal_test(model, i_snap, test_data, data, device)\n",
    "            \n",
    "    if verbose:\n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "    #print(f'Best Epoch: {best_epoch}')\n",
    "    \n",
    "    return best_model, avgpr_score_test, mrr_test, best_past_dict_1, best_past_dict_2, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f772f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def durendal_test(model, i_snap, test_data, data, device='cpu'):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    num_pos = len(test_data['node','follow','node'].edge_label_index[0])//2\n",
    "\n",
    "    h, *_ = model(test_data.x_dict, test_data.edge_index_dict, test_data['node','follow','node'].edge_label_index, i_snap)\n",
    "    fake, *_ = model(test_data.x_dict, test_data.edge_index_dict, test_data['node','follow','node'].corrupted_edge_index, i_snap)\n",
    "    \n",
    "    pred_cont = torch.sigmoid(h).cpu().detach().numpy()\n",
    "    fake_preds = torch.sigmoid(fake).cpu().detach().numpy()\n",
    "    \n",
    "    label = test_data['node','follow','node'].edge_label.cpu().detach().numpy()\n",
    "      \n",
    "    avgpr_score = average_precision_score(label, pred_cont)\n",
    "    mrr_score = compute_mrr(pred_cont[:num_pos], fake_preds)\n",
    "    \n",
    "    return avgpr_score, mrr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f32743",
   "metadata": {},
   "source": [
    "### TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38171ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_conv_1=256\n",
    "hidden_conv_2=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b471e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "device = torch.device('cuda')\n",
    "#torch.manual_seed(123456)\n",
    "#torch.cuda.manual_seed_all(123456)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrun = 10\n",
    "for run in range(nrun):\n",
    "    a, b, c = training(snapshots, hidden_conv_1, hidden_conv_2)\n",
    "    with open('results/durendal-rgcn-gru.txt','a') as wfile:\n",
    "        wfile.write(f'{a}\\n')\n",
    "    with open('results/durendal-han-gru.txt','a') as wfile:\n",
    "        wfile.write(f'{b}\\n')\n",
    "    with open('results/durendal-hgt-gru.txt','a') as wfile:\n",
    "        wfile.write(f'{c}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
