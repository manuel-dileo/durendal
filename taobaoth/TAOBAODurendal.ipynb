{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b0f534",
   "metadata": {},
   "source": [
    "Notebook to reproduce the results for TaobaoTH. To inspect annotated code, please see the notebook related to SteemitTH. Note that the code to run the experiments is the same for all the datasets; there are small changes just related to relation and model names. We decide to proceed in this way to treat the different prediction tasks separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc98a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, GRUCell\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "\n",
    "import random\n",
    "\n",
    "import bisect\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, structured_negative_sampling\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.transforms import RandomLinkSplit,NormalizeFeatures,Constant,OneHotDegree\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv,SAGEConv,GATv2Conv, GINConv, Linear, GCN, GAT\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a959a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taobao import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee235ca9",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd99646",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = get_taobao_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be230e52",
   "metadata": {},
   "source": [
    "# TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53e19c",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_insort(a, x, lo=0, hi=None):\n",
    "    \"\"\"Insert item x in list a, and keep it reverse-sorted assuming a\n",
    "    is reverse-sorted.\n",
    "\n",
    "    If x is already in a, insert it to the right of the rightmost x.\n",
    "\n",
    "    Optional args lo (default 0) and hi (default len(a)) bound the\n",
    "    slice of a to be searched.\n",
    "    \n",
    "    Function useful to compute MRR.\n",
    "    \"\"\"\n",
    "    if lo < 0:\n",
    "        raise ValueError('lo must be non-negative')\n",
    "    if hi is None:\n",
    "        hi = len(a)\n",
    "    while lo < hi:\n",
    "        mid = (lo+hi)//2\n",
    "        if x > a[mid]: hi = mid\n",
    "        else: lo = mid+1\n",
    "    a.insert(lo, x)\n",
    "    return lo\n",
    "\n",
    "def compute_mrr(real_scores, fake_scores):\n",
    "    srr = 0\n",
    "    count = 0\n",
    "    for i,score in enumerate(real_scores):\n",
    "        try:\n",
    "            fake_scores_cp = copy.copy([fake_scores[i]])\n",
    "        except IndexError: break\n",
    "        rank = reverse_insort(fake_scores_cp, score)\n",
    "        rr = 1/(rank+1) #index starts from zero\n",
    "        srr+=rr\n",
    "        count+=1\n",
    "    return srr/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    homdata = copy.deepcopy(snapshots[0]).to_homogeneous()\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in hetdata.x_dict.items()}\n",
    "    num_nodes = {node: len(v) for node, v in hetdata.x_dict.items()}\n",
    "    \n",
    "    in_channels_homo = homdata.x.size(1)\n",
    "    num_nodes_homo = homdata.x.size(0)\n",
    "    \n",
    "    #DURENDAL\n",
    "    durendal = TAOBAODurendal(in_channels, num_nodes, hetdata.metadata(),\n",
    "                        hidden_conv_1=hidden_conv_1,\n",
    "                        hidden_conv_2=hidden_conv_2)\n",
    "    \n",
    "    durendal.reset_parameters()\n",
    "    \n",
    "    durendalopt = torch.optim.Adam(params=durendal.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #GAT\n",
    "    gat = TAOBAOGAT(in_channels_homo, hidden_conv_1, hidden_conv_2)\n",
    "    gat.reset_parameters()\n",
    "    gatopt = torch.optim.Adam(params=gat.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #HAN\n",
    "    han = TAOBAOHAN(in_channels, hidden_conv_1, hidden_conv_2, hetdata.metadata())\n",
    "    han.reset_parameters()\n",
    "    hanopt = torch.optim.Adam(params=han.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #GConvGRU\n",
    "    gcgru = TAOBAOGConvGRU(in_channels_homo, hidden_conv_2)\n",
    "    gcgru.reset_parameters()\n",
    "    gcgruopt = torch.optim.Adam(params=gcgru.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #EvolveGCN\n",
    "    ev = TAOBAOEvolveGCN(in_channels_homo, num_nodes_homo)\n",
    "    ev.reset_parameters()\n",
    "    evopt = torch.optim.Adam(params=ev.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #HetEvolveGCN\n",
    "    hev = TAOBAOHEGCN(in_channels_homo, num_nodes_homo, list(hetdata.edge_index_dict.keys()))\n",
    "    hev.reset_parameters()\n",
    "    hevopt = torch.optim.Adam(params=hev.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    #ATU\n",
    "    atu = TAOBAOATU(in_channels, num_nodes, hetdata.metadata(),\n",
    "                        hidden_conv_1=hidden_conv_1,\n",
    "                        hidden_conv_2=hidden_conv_2)\n",
    "    atu.reset_parameters()\n",
    "    atuopt = torch.optim.Adam(params=atu.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    past_dict_1 = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_1[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_1[src][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_1[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[dst].num_nodes)])\n",
    "        \n",
    "    past_dict_2 = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_2[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_2[src][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_2[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[dst].num_nodes)])\n",
    "    \n",
    "    past_dict_1_atu = copy.deepcopy(past_dict_1)\n",
    "    past_dict_2_atu = copy.deepcopy(past_dict_2)\n",
    "    \n",
    "    durendal_avgpr = 0\n",
    "    durendal_mrr = 0\n",
    "    gat_avgpr = 0\n",
    "    gat_mrr = 0\n",
    "    han_avgpr = 0\n",
    "    han_mrr = 0\n",
    "    gcgru_avgpr = 0\n",
    "    gcgru_mrr = 0\n",
    "    ev_avgpr = 0\n",
    "    ev_mrr = 0\n",
    "    hev_avgpr = 0\n",
    "    hev_mrr = 0\n",
    "    atu_avgpr = 0\n",
    "    atu_mrr = 0\n",
    "    \n",
    "    H = None\n",
    "    H_1 = None\n",
    "    C_1 = None\n",
    "    H_2 = None\n",
    "    C_2 = None\n",
    "    \n",
    "    for i in range(num_snap-1):\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        hom_snapshot = snapshot.to_homogeneous()\n",
    "        hom_transform = RandomLinkSplit(num_val=0.0, num_test=0.20)\n",
    "        hom_train_data, _, hom_val_data = hom_transform(hom_snapshot)\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        hom_test_data = copy.deepcopy(snapshots[i+1]).to_homogeneous()\n",
    "        \"\"\"\n",
    "        hom_future_neg_edge_index = negative_sampling(\n",
    "            edge_index = hom_test_data.edge_index,\n",
    "            num_nodes = hom_test_data.num_nodes,\n",
    "            num_neg_samples = hom_test_data.edge_index.size(1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        #num_pos_edge = hom_test_data.edge_index.size(1)\n",
    "        hom_test_data.edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        #hom_test_data.edge_label_index = \\\n",
    "            #torch.cat([hom_test_data.edge_index, hom_future_neg_edge_index], dim=-1)\n",
    "        hom_test_data.edge_label_index = het_test_data['user','buy','item'].edge_label_index\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "        hom_val_data.corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_val_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        hom_test_data.corrupted_edge_index = corrupted_edge_index_test\n",
    "        het_test_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        durendal, dur_avgpr_test, dur_mrr_test , past_dict_1, past_dict_2, durendalopt =\\\n",
    "            durendal_train_single_snapshot(durendal, snapshot, i, het_train_data, het_val_data, het_test_data,\\\n",
    "                                  past_dict_1, past_dict_2, durendalopt)\n",
    "        \n",
    "        gat, gat_avgpr_test, gat_mrr_test, gatopt =\\\n",
    "            hom_train_single_snapshot(gat, snapshot, hom_train_data, hom_val_data, hom_test_data, gatopt)\n",
    "        \n",
    "        han, han_avgpr_test, han_mrr_test, hanopt =\\\n",
    "            het_train_single_snapshot(han, snapshot, het_train_data, het_val_data, het_test_data, hanopt)\n",
    "        \n",
    "        gcgru, gcgru_avgpr_test, gcgru_mrr_test, H, gcgruopt =\\\n",
    "            gcgru_train_single_snapshot(gcgru, snapshot, hom_train_data, hom_val_data, hom_test_data, gcgruopt, H)\n",
    "        \n",
    "        ev, ev_avgpr_test, ev_mrr_test, evopt =\\\n",
    "            hom_train_single_snapshot(ev, snapshot, hom_train_data, hom_val_data, hom_test_data, evopt)\n",
    "        \n",
    "        hev, hev_avgpr_test, hev_mrr_test, hevopt =\\\n",
    "            het_train_single_snapshot(hev, snapshot, het_train_data, het_val_data, het_test_data, hevopt)\n",
    "        \n",
    "        atu, atu_avgpr_test, atu_mrr_test , past_dict_1_atu, past_dict_2_atu, atuopt =\\\n",
    "            durendal_train_single_snapshot(atu, snapshot, i, het_train_data, het_val_data, het_test_data,\\\n",
    "                                  past_dict_1_atu, past_dict_2_atu, atuopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {i}\\n')\n",
    "        print(f' DURENDAL AVGPR Test: {dur_avgpr_test} \\n MRR Test: {dur_mrr_test}\\n')\n",
    "        print(f' GAT AVGPR Test: {gat_avgpr_test} \\n MRR Test: {gat_mrr_test}\\n')\n",
    "        print(f' HAN AVGPR Test: {han_avgpr_test} \\n MRR Test: {han_mrr_test}\\n')\n",
    "        print(f' GConvGRU AVGPR Test: {gcgru_avgpr_test} \\n MRR Test: {gcgru_mrr_test}\\n')\n",
    "        print(f' EvolveGCN AVGPR Test: {ev_avgpr_test} \\n MRR Test: {ev_mrr_test}\\n')\n",
    "        print(f' HetEvolveGCN AVGPR Test: {hev_avgpr_test} \\n MRR Test: {hev_mrr_test}\\n')\n",
    "        print(f' ATU AVGPR Test: {atu_avgpr_test} \\n MRR Test: {atu_mrr_test}\\n')\n",
    "        durendal_avgpr += dur_avgpr_test\n",
    "        durendal_mrr += dur_mrr_test\n",
    "        gat_avgpr += gat_avgpr_test\n",
    "        gat_mrr += gat_mrr_test\n",
    "        han_avgpr += han_avgpr_test\n",
    "        han_mrr += han_mrr_test\n",
    "        gcgru_avgpr += gcgru_avgpr_test\n",
    "        gcgru_mrr += gcgru_mrr_test\n",
    "        ev_avgpr += ev_avgpr_test\n",
    "        ev_mrr += ev_mrr_test\n",
    "        hev_avgpr += hev_avgpr_test\n",
    "        hev_mrr += hev_mrr_test\n",
    "        atu_avgpr += atu_avgpr_test\n",
    "        atu_mrr += atu_mrr_test\n",
    "        \n",
    "        \n",
    "    durendal_avgpr_all = durendal_avgpr / (num_snap-1)\n",
    "    durendal_mrr_all = durendal_mrr / (num_snap-1)\n",
    "    gat_avgpr_all = gat_avgpr / (num_snap-1)\n",
    "    gat_mrr_all = gat_mrr / (num_snap-1)\n",
    "    han_avgpr_all = han_avgpr / (num_snap-1)\n",
    "    han_mrr_all = han_mrr / (num_snap-1)\n",
    "    gcgru_avgpr_all = gcgru_avgpr / (num_snap-1)\n",
    "    gcgru_mrr_all = gcgru_mrr / (num_snap-1)\n",
    "    ev_avgpr_all = ev_avgpr / (num_snap-1)\n",
    "    ev_mrr_all = ev_mrr / (num_snap-1)\n",
    "    hev_avgpr_all = hev_avgpr / (num_snap-1)\n",
    "    hev_mrr_all = hev_mrr / (num_snap-1)\n",
    "    atu_avgpr_all = atu_avgpr / (num_snap-1)\n",
    "    atu_mrr_all = atu_mrr / (num_snap-1)\n",
    "    \n",
    "    print('DURENDAL')\n",
    "    print(f'\\tAVGPR over time: Test: {durendal_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {durendal_mrr_all}')\n",
    "    print()\n",
    "    print('GAT')\n",
    "    print(f'\\tAVGPR over time: Test: {gat_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {gat_mrr_all}')\n",
    "    print()\n",
    "    print('HAN')\n",
    "    print(f'\\tAVGPR over time: Test: {han_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {han_mrr_all}')\n",
    "    print()\n",
    "    print('GConvGRU')\n",
    "    print(f'\\tAVGPR over time: Test: {gcgru_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {gcgru_mrr_all}')\n",
    "    print()\n",
    "    print('EvolveGCN')\n",
    "    print(f'\\tAVGPR over time: Test: {ev_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {ev_mrr_all}')\n",
    "    print()\n",
    "    print('HetEvolveGCN')\n",
    "    print(f'\\tAVGPR over time: Test: {hev_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {hev_mrr_all}')\n",
    "    print('ATU')\n",
    "    print(f'\\tAVGPR over time: Test: {atu_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {atu_mrr_all}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d04690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_han(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in hetdata.x_dict.items()}\n",
    "    num_nodes = {node: len(v) for node, v in hetdata.x_dict.items()}\n",
    "    \n",
    "    #HAN\n",
    "    han = TAOBAOHAN(in_channels, hidden_conv_1, hidden_conv_2, hetdata.metadata())\n",
    "    han.reset_parameters()\n",
    "    hanopt = torch.optim.Adam(params=han.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    han_avgpr = 0\n",
    "    han_mrr = 0\n",
    "    \n",
    "    ch=0\n",
    "    for i in range(num_snap-1):\n",
    "        #if ch>=9:break\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "    \n",
    "        het_val_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_test_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        \n",
    "        han, han_avgpr_test, han_mrr_test, hanopt =\\\n",
    "            het_train_single_snapshot(han, snapshot, het_train_data, het_val_data, het_test_data, hanopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {i}\\n')\n",
    "        print(f' HAN AVGPR Test: {han_avgpr_test} \\n MRR Test: {han_mrr_test}\\n')\n",
    "        han_avgpr += han_avgpr_test\n",
    "        han_mrr += han_mrr_test\n",
    "        ch+=1\n",
    "        \n",
    "        \n",
    "    han_avgpr_all = han_avgpr / ch\n",
    "    han_mrr_all = han_mrr / ch\n",
    "    \n",
    "    print('HAN')\n",
    "    print(f'\\tAVGPR over time: Test: {han_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {han_mrr_all}')\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_gat(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    homdata = copy.deepcopy(snapshots[0]).to_homogeneous()\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels_homo = homdata.x.size(1)\n",
    "    num_nodes_homo = homdata.x.size(0)\n",
    "    \n",
    "    #GAT\n",
    "    gat = TAOBAOGAT(in_channels_homo, hidden_conv_1, hidden_conv_2)\n",
    "    gat.reset_parameters()\n",
    "    gatopt = torch.optim.Adam(params=gat.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    gat_avgpr = 0\n",
    "    gat_mrr = 0\n",
    "    \n",
    "    ch=0\n",
    "    for i in range(num_snap-1):\n",
    "        if ch>=9:break\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        hom_snapshot = snapshot.to_homogeneous()\n",
    "        hom_transform = RandomLinkSplit(num_val=0.0, num_test=0.20)\n",
    "        hom_train_data, _, hom_val_data = hom_transform(hom_snapshot)\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        hom_test_data = copy.deepcopy(snapshots[i+1]).to_homogeneous()\n",
    "        #num_pos_edge = hom_test_data.edge_index.size(1)\n",
    "        hom_test_data.edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        #hom_test_data.edge_label_index = \\\n",
    "            #torch.cat([hom_test_data.edge_index, hom_future_neg_edge_index], dim=-1)\n",
    "        hom_test_data.edge_label_index = het_test_data['user','buy','item'].edge_label_index\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "        hom_val_data.corrupted_edge_index = corrupted_edge_index_val\n",
    "        hom_test_data.corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        \n",
    "        gat, gat_avgpr_test, gat_mrr_test, gatopt =\\\n",
    "            hom_train_single_snapshot(gat, snapshot, hom_train_data, hom_val_data, hom_test_data, gatopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {ch}\\n')\n",
    "        print(f' GAT AVGPR Test: {gat_avgpr_test} \\n MRR Test: {gat_mrr_test}\\n')\n",
    "        gat_avgpr += gat_avgpr_test\n",
    "        gat_mrr += gat_mrr_test\n",
    "        ch+=1\n",
    "        \n",
    "    gat_avgpr_all = gat_avgpr / ch\n",
    "    gat_mrr_all = gat_mrr / ch\n",
    "    \n",
    "    print('GAT')\n",
    "    print(f'\\tAVGPR over time: Test: {gat_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {gat_mrr_all}')\n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c417364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_durendal(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = 10\n",
    "    first_snap = snapshots[0]\n",
    "    edge_types = list(first_snap.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in first_snap.x_dict.items()}\n",
    "    num_nodes = {node: len(v) for node, v in first_snap.x_dict.items()}\n",
    "    \n",
    "    #DURENDAL\n",
    "    durendal = TAOBAODurendal(in_channels, num_nodes, first_snap.metadata(),\n",
    "                        hidden_conv_1=hidden_conv_1,\n",
    "                        hidden_conv_2=hidden_conv_2)\n",
    "    \n",
    "    durendal.reset_parameters()\n",
    "    \n",
    "    durendalopt = torch.optim.Adam(params=durendal.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    past_dict_1 = {}\n",
    "    for node in first_snap.x_dict.keys():\n",
    "        past_dict_1[node] = {}\n",
    "    for src,r,dst in first_snap.edge_index_dict.keys():\n",
    "        past_dict_1[src][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(first_snap[src].num_nodes)])\n",
    "        past_dict_1[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(first_snap[dst].num_nodes)])\n",
    "        \n",
    "    past_dict_2 = {}\n",
    "    for node in first_snap.x_dict.keys():\n",
    "        past_dict_2[node] = {}\n",
    "    for src,r,dst in first_snap.edge_index_dict.keys():\n",
    "        past_dict_2[src][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(first_snap[src].num_nodes)])\n",
    "        past_dict_2[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(first_snap[dst].num_nodes)])\n",
    "    \n",
    "    durendal_avgpr = 0\n",
    "    durendal_mrr = 0\n",
    "    \n",
    "    del(first_snap)\n",
    "    gc.collect()\n",
    "    \n",
    "    ch=0\n",
    "    for i in range(num_snap-1):\n",
    "        #if ch >= 24: break\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "       \n",
    "        het_val_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_test_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        durendal, dur_avgpr_test, dur_mrr_test , past_dict_1, past_dict_2, durendalopt =\\\n",
    "            durendal_train_single_snapshot(durendal, snapshot, i, het_train_data, het_val_data, het_test_data,\\\n",
    "                                  past_dict_1, past_dict_2, durendalopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {ch}\\n')\n",
    "        print(f' DURENDAL AVGPR Test: {dur_avgpr_test} \\n MRR Test: {dur_mrr_test}\\n')\n",
    "        durendal_avgpr += dur_avgpr_test\n",
    "        durendal_mrr += dur_mrr_test\n",
    "        ch+=1\n",
    "        \n",
    "        \n",
    "    #durendal_avgpr_all = durendal_avgpr / (num_snap-1)\n",
    "    #durendal_mrr_all = durendal_mrr / (num_snap-1)\n",
    "    durendal_avgpr_all = durendal_avgpr / (ch)\n",
    "    durendal_mrr_all = durendal_mrr / (ch)\n",
    "    \n",
    "    print('DURENDAL')\n",
    "    print(f'\\tAVGPR over time: Test: {durendal_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {durendal_mrr_all}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_gconvgru(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    homdata = copy.deepcopy(snapshots[0]).to_homogeneous()\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in hetdata.x_dict.items()}\n",
    "    num_nodes = {node: len(v) for node, v in hetdata.x_dict.items()}\n",
    "    \n",
    "    in_channels_homo = homdata.x.size(1)\n",
    "    num_nodes_homo = homdata.x.size(0)\n",
    "    \n",
    "    #GConvGRU\n",
    "    gcgru = TAOBAOGConvGRU(in_channels_homo, hidden_conv_2)\n",
    "    gcgru.reset_parameters()\n",
    "    gcgruopt = torch.optim.Adam(params=gcgru.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    past_dict_1 = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_1[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_1[src][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_1[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[dst].num_nodes)])\n",
    "        \n",
    "    past_dict_2 = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_2[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_2[src][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_2[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[dst].num_nodes)])\n",
    "    \n",
    "    gcgru_avgpr = 0\n",
    "    gcgru_mrr = 0\n",
    "    \n",
    "    H = None\n",
    "    H_1 = None\n",
    "    C_1 = None\n",
    "    H_2 = None\n",
    "    C_2 = None\n",
    "    \n",
    "    ch=0\n",
    "    for i in range(num_snap-1):\n",
    "        if ch>=9: break\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        hom_snapshot = snapshot.to_homogeneous()\n",
    "        hom_transform = RandomLinkSplit(num_val=0.0, num_test=0.20)\n",
    "        hom_train_data, _, hom_val_data = hom_transform(hom_snapshot)\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        hom_test_data = copy.deepcopy(snapshots[i+1]).to_homogeneous()\n",
    "        \"\"\"\n",
    "        hom_future_neg_edge_index = negative_sampling(\n",
    "            edge_index = hom_test_data.edge_index,\n",
    "            num_nodes = hom_test_data.num_nodes,\n",
    "            num_neg_samples = hom_test_data.edge_index.size(1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        #num_pos_edge = hom_test_data.edge_index.size(1)\n",
    "        hom_test_data.edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        #hom_test_data.edge_label_index = \\\n",
    "            #torch.cat([hom_test_data.edge_index, hom_future_neg_edge_index], dim=-1)\n",
    "        hom_test_data.edge_label_index = het_test_data['user','buy','item'].edge_label_index\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "        hom_val_data.corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_val_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        hom_test_data.corrupted_edge_index = corrupted_edge_index_test\n",
    "        het_test_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        \n",
    "        gcgru, gcgru_avgpr_test, gcgru_mrr_test, H, gcgruopt =\\\n",
    "            gcgru_train_single_snapshot(gcgru, snapshot, hom_train_data, hom_val_data, hom_test_data, gcgruopt, H)\n",
    "\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {i}\\n')\n",
    "        print(f' GConvGRU AVGPR Test: {gcgru_avgpr_test} \\n MRR Test: {gcgru_mrr_test}\\n')\n",
    "        gcgru_avgpr += gcgru_avgpr_test\n",
    "        gcgru_mrr += gcgru_mrr_test\n",
    "        ch+=1\n",
    "        \n",
    "        \n",
    "    gcgru_avgpr_all = gcgru_avgpr / ch\n",
    "    gcgru_mrr_all = gcgru_mrr / ch\n",
    "    \n",
    "    \n",
    "    print('GConvGRU')\n",
    "    print(f'\\tAVGPR over time: Test: {gcgru_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {gcgru_mrr_all}')\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8480a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_ev(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    homdata = copy.deepcopy(snapshots[0]).to_homogeneous()\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in hetdata.x_dict.items()}\n",
    "    num_nodes = {node: len(v) for node, v in hetdata.x_dict.items()}\n",
    "    \n",
    "    in_channels_homo = homdata.x.size(1)\n",
    "    num_nodes_homo = homdata.x.size(0)\n",
    "    \n",
    "    #EvolveGCN\n",
    "    ev = TAOBAOEvolveGCN(in_channels_homo, num_nodes_homo)\n",
    "    ev.reset_parameters()\n",
    "    evopt = torch.optim.Adam(params=ev.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "        \n",
    "    ev_avgpr = 0\n",
    "    ev_mrr = 0\n",
    "    \n",
    "    H = None\n",
    "    H_1 = None\n",
    "    C_1 = None\n",
    "    H_2 = None\n",
    "    C_2 = None\n",
    "    \n",
    "    ch=0\n",
    "    for i in range(num_snap-1):\n",
    "        if ch >= 9: break\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        hom_snapshot = snapshot.to_homogeneous()\n",
    "        hom_transform = RandomLinkSplit(num_val=0.0, num_test=0.20)\n",
    "        hom_train_data, _, hom_val_data = hom_transform(hom_snapshot)\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        hom_test_data = copy.deepcopy(snapshots[i+1]).to_homogeneous()\n",
    "        \"\"\"\n",
    "        hom_future_neg_edge_index = negative_sampling(\n",
    "            edge_index = hom_test_data.edge_index,\n",
    "            num_nodes = hom_test_data.num_nodes,\n",
    "            num_neg_samples = hom_test_data.edge_index.size(1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        #num_pos_edge = hom_test_data.edge_index.size(1)\n",
    "        hom_test_data.edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        #hom_test_data.edge_label_index = \\\n",
    "            #torch.cat([hom_test_data.edge_index, hom_future_neg_edge_index], dim=-1)\n",
    "        hom_test_data.edge_label_index = het_test_data['user','buy','item'].edge_label_index\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "        hom_val_data.corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_val_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        hom_test_data.corrupted_edge_index = corrupted_edge_index_test\n",
    "        het_test_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        ev, ev_avgpr_test, ev_mrr_test, evopt =\\\n",
    "            hom_train_single_snapshot(ev, snapshot, hom_train_data, hom_val_data, hom_test_data, evopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {i}\\n')\n",
    "        print(f' EvolveGCN AVGPR Test: {ev_avgpr_test} \\n MRR Test: {ev_mrr_test}\\n')\n",
    "        ev_avgpr += ev_avgpr_test\n",
    "        ev_mrr += ev_mrr_test\n",
    "        ch+=1\n",
    "        \n",
    "    ev_avgpr_all = ev_avgpr / ch\n",
    "    ev_mrr_all = ev_mrr / ch\n",
    "    \n",
    "    print('EvolveGCN')\n",
    "    print(f'\\tAVGPR over time: Test: {ev_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {ev_mrr_all}')\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04519b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_hev(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    homdata = copy.deepcopy(snapshots[0]).to_homogeneous()\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in hetdata.x_dict.items()}\n",
    "    num_nodes = {node: len(v) for node, v in hetdata.x_dict.items()}\n",
    "    \n",
    "    in_channels_homo = homdata.x.size(1)\n",
    "    num_nodes_homo = homdata.x.size(0)\n",
    "    \n",
    "    #HetEvolveGCN\n",
    "    hev = TAOBAOHEGCN(in_channels_homo, num_nodes_homo, list(hetdata.edge_index_dict.keys()))\n",
    "    hev.reset_parameters()\n",
    "    hevopt = torch.optim.Adam(params=hev.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    hev_avgpr = 0\n",
    "    hev_mrr = 0\n",
    "    \n",
    "    H = None\n",
    "    H_1 = None\n",
    "    C_1 = None\n",
    "    H_2 = None\n",
    "    C_2 = None\n",
    "    \n",
    "    ch=0\n",
    "    for i in range(num_snap-1):\n",
    "        if ch>=9: break\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        hom_snapshot = snapshot.to_homogeneous()\n",
    "        hom_transform = RandomLinkSplit(num_val=0.0, num_test=0.20)\n",
    "        hom_train_data, _, hom_val_data = hom_transform(hom_snapshot)\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        hom_test_data = copy.deepcopy(snapshots[i+1]).to_homogeneous()\n",
    "        \"\"\"\n",
    "        hom_future_neg_edge_index = negative_sampling(\n",
    "            edge_index = hom_test_data.edge_index,\n",
    "            num_nodes = hom_test_data.num_nodes,\n",
    "            num_neg_samples = hom_test_data.edge_index.size(1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        #num_pos_edge = hom_test_data.edge_index.size(1)\n",
    "        hom_test_data.edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        #hom_test_data.edge_label_index = \\\n",
    "            #torch.cat([hom_test_data.edge_index, hom_future_neg_edge_index], dim=-1)\n",
    "        hom_test_data.edge_label_index = het_test_data['user','buy','item'].edge_label_index\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "        hom_val_data.corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_val_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        hom_test_data.corrupted_edge_index = corrupted_edge_index_test\n",
    "        het_test_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        \n",
    "        hev, hev_avgpr_test, hev_mrr_test, hevopt =\\\n",
    "            het_train_single_snapshot(hev, snapshot, het_train_data, het_val_data, het_test_data, hevopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {i}\\n')\n",
    "        print(f' HetEvolveGCN AVGPR Test: {hev_avgpr_test} \\n MRR Test: {hev_mrr_test}\\n')\n",
    "        print(f' ATU AVGPR Test: {atu_avgpr_test} \\n MRR Test: {atu_mrr_test}\\n')\n",
    "        hev_avgpr += hev_avgpr_test\n",
    "        hev_mrr += hev_mrr_test\n",
    "        ch+=1\n",
    "        \n",
    "    hev_avgpr_all = hev_avgpr / (ch)\n",
    "    hev_mrr_all = hev_mrr / (ch)\n",
    "    print('HetEvolveGCN')\n",
    "    print(f'\\tAVGPR over time: Test: {hev_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {hev_mrr_all}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_atu(snapshots, hidden_conv_1, hidden_conv_2, device='cpu'):\n",
    "    num_snap = len(snapshots)\n",
    "    hetdata = copy.deepcopy(snapshots[0])\n",
    "    homdata = copy.deepcopy(snapshots[0]).to_homogeneous()\n",
    "    edge_types = list(hetdata.edge_index_dict.keys())\n",
    "    \n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-3\n",
    "    \n",
    "    in_channels = {node: len(v[0]) for node,v in hetdata.x_dict.items()}\n",
    "    num_nodes = {node: len(v) for node, v in hetdata.x_dict.items()}\n",
    "    \n",
    "    in_channels_homo = homdata.x.size(1)\n",
    "    num_nodes_homo = homdata.x.size(0)\n",
    "    \n",
    "    #ATU\n",
    "    atu = TAOBAOATU(in_channels, num_nodes, hetdata.metadata(),\n",
    "                        hidden_conv_1=hidden_conv_1,\n",
    "                        hidden_conv_2=hidden_conv_2)\n",
    "    atu.reset_parameters()\n",
    "    atuopt = torch.optim.Adam(params=atu.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    past_dict_1_atu = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_1_atu[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_1_atu[src][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_1_atu[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_1)] for i in range(hetdata[dst].num_nodes)])\n",
    "        \n",
    "    past_dict_2_atu = {}\n",
    "    for node in hetdata.x_dict.keys():\n",
    "        past_dict_2_atu[node] = {}\n",
    "    for src,r,dst in hetdata.edge_index_dict.keys():\n",
    "        past_dict_2_atu[src][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[src].num_nodes)])\n",
    "        past_dict_2_atu[dst][r] = torch.Tensor([[0 for j in range(hidden_conv_2)] for i in range(hetdata[dst].num_nodes)])\n",
    "    \n",
    "    atu_avgpr = 0\n",
    "    atu_mrr = 0\n",
    "    \n",
    "    ch=0\n",
    "    for i in range(num_snap-1):\n",
    "        if ch>=9: break\n",
    "        #CREATE TRAIN + VAL + TEST SET FOR THE CURRENT SNAP\n",
    "        snapshot = copy.deepcopy(snapshots[i])\n",
    "        hom_snapshot = snapshot.to_homogeneous()\n",
    "        hom_transform = RandomLinkSplit(num_val=0.0, num_test=0.20)\n",
    "        hom_train_data, _, hom_val_data = hom_transform(hom_snapshot)\n",
    "        \n",
    "        het_transform = RandomLinkSplit(num_val=0.0,num_test=0.20, edge_types=edge_types)\n",
    "        het_train_data, _, het_val_data = het_transform(snapshot)\n",
    "     \n",
    "        het_test_data = copy.deepcopy(snapshots[i+1])\n",
    "        het_future_neg_edge_index = negative_sampling(\n",
    "            edge_index=het_test_data['user','buy','item'].edge_index, #positive edges\n",
    "            num_nodes=len(het_test_data['user'].x), # number of nodes\n",
    "            num_neg_samples=het_test_data['user','buy','item'].edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "        #edge index ok, edge_label concat, edge_label_index concat\n",
    "        num_pos_edge = het_test_data['user','buy','item'].edge_index.size(1)\n",
    "        het_test_data['user','buy','item'].edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        het_test_data['user','buy','item'].edge_label_index = \\\n",
    "            torch.cat([het_test_data['user','buy','item'].edge_index, het_future_neg_edge_index], dim=-1)\n",
    "        \n",
    "        hom_test_data = copy.deepcopy(snapshots[i+1]).to_homogeneous()\n",
    "        \"\"\"\n",
    "        hom_future_neg_edge_index = negative_sampling(\n",
    "            edge_index = hom_test_data.edge_index,\n",
    "            num_nodes = hom_test_data.num_nodes,\n",
    "            num_neg_samples = hom_test_data.edge_index.size(1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        #num_pos_edge = hom_test_data.edge_index.size(1)\n",
    "        hom_test_data.edge_label = torch.Tensor(\\\n",
    "            np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "        #hom_test_data.edge_label_index = \\\n",
    "            #torch.cat([hom_test_data.edge_index, hom_future_neg_edge_index], dim=-1)\n",
    "        hom_test_data.edge_label_index = het_test_data['user','buy','item'].edge_label_index\n",
    "        \n",
    "        #corrupted_edges as field of test_data and val_data\n",
    "        src_t, _, corrupted_dst =\\\n",
    "            structured_negative_sampling(het_val_data['user','buy','item'].edge_index)\n",
    "            \n",
    "        corrupted_edge_index_val = torch.stack([src_t, corrupted_dst])\n",
    "        \n",
    "        src_t_test, _, corrupted_dst_test =\\\n",
    "            structured_negative_sampling(het_test_data['user','buy','item'].edge_index)\n",
    "        \n",
    "        corrupted_edge_index_test = torch.stack([src_t_test, corrupted_dst_test])\n",
    "        \n",
    "        hom_val_data.corrupted_edge_index = corrupted_edge_index_val\n",
    "        het_val_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_val\n",
    "        hom_test_data.corrupted_edge_index = corrupted_edge_index_test\n",
    "        het_test_data['user','buy','item'].corrupted_edge_index = corrupted_edge_index_test\n",
    "        \n",
    "        #TRAIN AND TEST THE MODEL FOR THE CURRENT SNAP\n",
    "        atu, atu_avgpr_test, atu_mrr_test , past_dict_1_atu, past_dict_2_atu, atuopt =\\\n",
    "            durendal_train_single_snapshot(atu, snapshot, i, het_train_data, het_val_data, het_test_data,\\\n",
    "                                  past_dict_1_atu, past_dict_2_atu, atuopt)\n",
    "        \n",
    "        #SAVE AND DISPLAY EVALUATION\n",
    "        print(f'Snapshot: {i}\\n')\n",
    "        print(f' ATU AVGPR Test: {atu_avgpr_test} \\n MRR Test: {atu_mrr_test}\\n')\n",
    "        atu_avgpr += atu_avgpr_test\n",
    "        atu_mrr += atu_mrr_test\n",
    "        ch+=1\n",
    "        \n",
    "    atu_avgpr_all = atu_avgpr / ch\n",
    "    atu_mrr_all = atu_mrr / ch\n",
    "\n",
    "    \n",
    "    print('ATU')\n",
    "    print(f'\\tAVGPR over time: Test: {atu_avgpr_all}')\n",
    "    print(f'\\tMRR over time: Test: {atu_mrr_all}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2be8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def durendal_train_single_snapshot(model, data, i_snap, train_data, val_data, test_data,\\\n",
    "                          past_dict_1, past_dict_2,\\\n",
    "                          optimizer, device='cpu', num_epochs=50, verbose=False):\n",
    "    \n",
    "    mrr_val_max = 0\n",
    "    avgpr_val_max = 0\n",
    "    best_model = model\n",
    "    train_data = train_data.to(device)\n",
    "    best_epoch = -1\n",
    "    best_past_dict_1 = {}\n",
    "    best_past_dict_2 = {}\n",
    "    \n",
    "    tol = 5e-2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        ## Note\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Compute loss and backpropagate\n",
    "        ## 3. Update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        pred, past_dict_1, past_dict_2 =\\\n",
    "            model(train_data.x_dict, train_data.edge_index_dict, train_data['user','buy','item'].edge_label_index,\\\n",
    "                  i_snap, past_dict_1, past_dict_2)\n",
    "        \n",
    "        loss = model.loss(pred, train_data['user','buy','item'].edge_label.type_as(pred)) #loss to fine tune on current snapshot\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        log = 'Epoch: {:03d}\\n AVGPR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n MRR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n F1-Score Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n Loss: {}'\n",
    "        avgpr_score_val, mrr_val = durendal_test(model, i_snap, val_data, data, device)\n",
    "        \n",
    "        \"\"\"\n",
    "        if mrr_val_max-tol < mrr_val:\n",
    "            mrr_val_max = mrr_val\n",
    "            best_epoch = epoch\n",
    "            best_current_embeddings = current_embeddings\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        #print(f'Epoch: {epoch} done')\n",
    "            \n",
    "        \"\"\"\n",
    "        if avgpr_val_max-tol <= avgpr_score_val:\n",
    "            avgpr_val_max = avgpr_score_val\n",
    "            best_epoch = epoch\n",
    "            best_past_dict_1 = past_dict_1\n",
    "            best_past_dict_2 = past_dict_2\n",
    "            best_model = model\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    avgpr_score_test, mrr_test = durendal_test(model, i_snap, test_data, data, device)\n",
    "            \n",
    "    if verbose:\n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "    #print(f'Best Epoch: {best_epoch}')\n",
    "    \n",
    "    return best_model, avgpr_score_test, mrr_test, best_past_dict_1, best_past_dict_2, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hom_train_single_snapshot(model, data, train_data, val_data, test_data,\\\n",
    "                          optimizer, device='cpu', num_epochs=50, verbose=False):\n",
    "    \n",
    "    mrr_val_max = 0\n",
    "    avgpr_val_max = 0\n",
    "    best_model = model\n",
    "    train_data = train_data.to(device)\n",
    "    best_epoch = -1\n",
    "    \n",
    "    tol = 5e-2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        ## Note\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Compute loss and backpropagate\n",
    "        ## 3. Update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        pred = model(train_data.x, train_data.edge_index, train_data.edge_label_index)\n",
    "        \n",
    "        loss = model.loss(pred, train_data.edge_label.type_as(pred)) #loss to fine tune on current snapshot\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        log = 'Epoch: {:03d}\\n AVGPR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n MRR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n F1-Score Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n Loss: {}'\n",
    "        avgpr_score_val, mrr_val = hom_test(model, val_data, data, device)\n",
    "        \n",
    "        \"\"\"\n",
    "        if mrr_val_max-tol < mrr_val:\n",
    "            mrr_val_max = mrr_val\n",
    "            best_epoch = epoch\n",
    "            best_current_embeddings = current_embeddings\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        #print(f'Epoch: {epoch} done')\n",
    "            \n",
    "        \"\"\"\n",
    "        if avgpr_val_max-tol <= avgpr_score_val:\n",
    "            avgpr_val_max = avgpr_score_val\n",
    "            best_epoch = epoch\n",
    "            best_model = model\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    avgpr_score_test, mrr_test = hom_test(model, test_data, data, device)\n",
    "            \n",
    "    if verbose:\n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "    #print(f'Best Epoch: {best_epoch}')\n",
    "    \n",
    "    return best_model, avgpr_score_test, mrr_test, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc165c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def het_train_single_snapshot(model, data, train_data, val_data, test_data,\\\n",
    "                          optimizer, device='cpu', num_epochs=50, verbose=False):\n",
    "    \n",
    "    mrr_val_max = 0\n",
    "    avgpr_val_max = 0\n",
    "    best_model = model\n",
    "    train_data = train_data.to(device)\n",
    "    best_epoch = -1\n",
    "    \n",
    "    tol = 5e-2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        ## Note\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Compute loss and backpropagate\n",
    "        ## 3. Update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        pred =\\\n",
    "            model(train_data.x_dict, train_data.edge_index_dict, train_data['user','buy','item'].edge_label_index)\n",
    "        \n",
    "        loss = model.loss(pred, train_data['user','buy','item'].edge_label.type_as(pred)) #loss to fine tune on current snapshot\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        log = 'Epoch: {:03d}\\n AVGPR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n MRR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n F1-Score Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n Loss: {}'\n",
    "        avgpr_score_val, mrr_val = het_test(model, val_data, data, device)\n",
    "        \n",
    "        \"\"\"\n",
    "        if mrr_val_max-tol < mrr_val:\n",
    "            mrr_val_max = mrr_val\n",
    "            best_epoch = epoch\n",
    "            best_current_embeddings = current_embeddings\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        #print(f'Epoch: {epoch} done')\n",
    "            \n",
    "        \"\"\"\n",
    "        if avgpr_val_max-tol <= avgpr_score_val:\n",
    "            avgpr_val_max = avgpr_score_val\n",
    "            best_epoch = epoch\n",
    "            best_model = model\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    avgpr_score_test, mrr_test = het_test(model, test_data, data, device)\n",
    "            \n",
    "    if verbose:\n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "    #print(f'Best Epoch: {best_epoch}')\n",
    "    \n",
    "    return best_model, avgpr_score_test, mrr_test, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcgru_train_single_snapshot(model, data, train_data, val_data, test_data,\\\n",
    "                          optimizer, H=None, device='cpu', num_epochs=50, verbose=False):\n",
    "    \n",
    "    mrr_val_max = 0\n",
    "    avgpr_val_max = 0\n",
    "    best_model = model\n",
    "    train_data = train_data.to(device)\n",
    "    best_epoch = -1\n",
    "    \n",
    "    tol = 5e-2\n",
    "    \n",
    "    best_H = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        ## Note\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Compute loss and backpropagate\n",
    "        ## 3. Update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #H = None\n",
    "            \n",
    "        pred, H = model(train_data.x, train_data.edge_index, train_data.edge_label_index, H)\n",
    "        \n",
    "        loss = model.loss(pred, train_data.edge_label.type_as(pred)) #loss to fine tune on current snapshot\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        log = 'Epoch: {:03d}\\n AVGPR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n MRR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n F1-Score Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n Loss: {}'\n",
    "        avgpr_score_val, mrr_val = gcgru_test(model, val_data, data, device)\n",
    "        \n",
    "        \"\"\"\n",
    "        if mrr_val_max-tol < mrr_val:\n",
    "            mrr_val_max = mrr_val\n",
    "            best_epoch = epoch\n",
    "            best_current_embeddings = current_embeddings\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        #print(f'Epoch: {epoch} done')\n",
    "            \n",
    "        \"\"\"\n",
    "        if avgpr_val_max-tol <= avgpr_score_val:\n",
    "            avgpr_val_max = avgpr_score_val\n",
    "            best_H = H.clone()\n",
    "            best_epoch = epoch\n",
    "            best_model = model\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    avgpr_score_test, mrr_test = gcgru_test(model, test_data, data, device)\n",
    "            \n",
    "    if verbose:\n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "    #print(f'Best Epoch: {best_epoch}')\n",
    "    \n",
    "    return best_model, avgpr_score_test, mrr_test, best_H, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def htlstm_train_single_snapshot(model, data, train_data, val_data, test_data,\\\n",
    "                                 H_1, C_1, H_2, C_2, optimizer, device='cpu', num_epochs=50, verbose=False):\n",
    "    mrr_val_max = 0\n",
    "    avgpr_val_max = 0\n",
    "    best_model = model\n",
    "    train_data = train_data.to(device)\n",
    "    best_epoch = -1\n",
    "    best_H_1 = None\n",
    "    best_H_2 = None\n",
    "    best_C_1 = None\n",
    "    best_C_2 = None\n",
    "    \n",
    "    tol = 5e-2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        ## Note\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Compute loss and backpropagate\n",
    "        ## 3. Update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        pred, cH_1, cC_1, cH_2, cC_2 =\\\n",
    "            model(train_data.x_dict, train_data.edge_index_dict, train_data['user','buy','item'].edge_label_index,\\\n",
    "                  H_1, C_1, H_2, C_2)\n",
    "        \n",
    "        loss = model.loss(pred, train_data['user','buy','item'].edge_label.type_as(pred)) #loss to fine tune on current snapshot\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        log = 'Epoch: {:03d}\\n AVGPR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n MRR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n F1-Score Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n Loss: {}'\n",
    "        avgpr_score_val, mrr_val = htlstm_test(model, val_data, data, H_1, C_1, H_2, C_2, device)\n",
    "        \n",
    "        \"\"\"\n",
    "        if mrr_val_max-tol < mrr_val:\n",
    "            mrr_val_max = mrr_val\n",
    "            best_epoch = epoch\n",
    "            best_current_embeddings = current_embeddings\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        #print(f'Epoch: {epoch} done')\n",
    "            \n",
    "        \"\"\"\n",
    "        if avgpr_val_max-tol <= avgpr_score_val:\n",
    "            avgpr_val_max = avgpr_score_val\n",
    "            best_epoch = epoch\n",
    "            best_H_1 = cH_1\n",
    "            best_H_2 = cH_2\n",
    "            best_C_1 = cC_1\n",
    "            best_C_2 = cC_2\n",
    "            best_model = model\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    avgpr_score_test, mrr_test =  htlstm_test(model, test_data, data, H_1, C_1, H_2, C_2, device)\n",
    "            \n",
    "    if verbose:\n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "    #print(f'Best Epoch: {best_epoch}')\n",
    "    \n",
    "    return best_model, avgpr_score_test, mrr_test, best_H_1, best_C_1, best_H_2, best_C_2, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def durendal_test(model, i_snap, test_data, data, device='cpu'):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    num_pos = (len(test_data['user','buy','item'].edge_label_index[0])//2)\n",
    "\n",
    "    h, *_ = model(test_data.x_dict, test_data.edge_index_dict, test_data['user','buy','item'].edge_label_index, i_snap)\n",
    "    fake, *_ = model(test_data.x_dict, test_data.edge_index_dict, test_data['user','buy','item'].corrupted_edge_index, i_snap)\n",
    "    \n",
    "    pred_cont = torch.sigmoid(h).cpu().detach().numpy()\n",
    "    fake_preds = torch.sigmoid(fake).cpu().detach().numpy()\n",
    "    \n",
    "    label = test_data['user','buy','item'].edge_label.cpu().detach().numpy()\n",
    "      \n",
    "    avgpr_score = average_precision_score(label, pred_cont)\n",
    "    mrr_score = compute_mrr(pred_cont[:num_pos], fake_preds)\n",
    "    \n",
    "    return avgpr_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac693c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hom_test(model, test_data, data, device='cpu'):\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "\n",
    "    num_pos = (len(test_data.edge_label_index[0])//2)\n",
    "\n",
    "    h = model(test_data.x, test_data.edge_index, test_data.edge_label_index)\n",
    "    fake = model(test_data.x, test_data.edge_index, test_data.corrupted_edge_index)\n",
    "    \n",
    "    pred_cont = torch.sigmoid(h).cpu().detach().numpy()\n",
    "    fake_preds = torch.sigmoid(fake).cpu().detach().numpy()\n",
    "    \n",
    "    label = test_data.edge_label.cpu().detach().numpy()\n",
    "      \n",
    "    avgpr_score = average_precision_score(label, pred_cont)\n",
    "    mrr_score = compute_mrr(pred_cont[:num_pos], fake_preds)\n",
    "    \n",
    "    return avgpr_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def het_test(model, test_data, data, device='cpu'):\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    num_pos = (len(test_data['user','buy','item'].edge_label_index[0])//2)\n",
    "\n",
    "    h = model(test_data.x_dict, test_data.edge_index_dict, test_data['user','buy','item'].edge_label_index)\n",
    "    fake = model(test_data.x_dict, test_data.edge_index_dict, test_data['user','buy','item'].corrupted_edge_index)\n",
    "    \n",
    "    pred_cont = torch.sigmoid(h).cpu().detach().numpy()\n",
    "    fake_preds = torch.sigmoid(fake).cpu().detach().numpy()\n",
    "    \n",
    "    label = test_data['user','buy','item'].edge_label.cpu().detach().numpy()\n",
    "      \n",
    "    avgpr_score = average_precision_score(label, pred_cont)\n",
    "    mrr_score = compute_mrr(pred_cont[:num_pos], fake_preds)\n",
    "    \n",
    "    return avgpr_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b667f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcgru_test(model, test_data, data, device='cpu'):\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    num_pos = (len(test_data.edge_label_index[0])//2)\n",
    "\n",
    "    h, _ = model(test_data.x, test_data.edge_index, test_data.edge_label_index)\n",
    "    fake, _ = model(test_data.x, test_data.edge_index, test_data.corrupted_edge_index)\n",
    "    \n",
    "    pred_cont = torch.sigmoid(h).cpu().detach().numpy()\n",
    "    fake_preds = torch.sigmoid(fake).cpu().detach().numpy()\n",
    "    \n",
    "    label = test_data.edge_label.cpu().detach().numpy()\n",
    "      \n",
    "    avgpr_score = average_precision_score(label, pred_cont)\n",
    "    mrr_score = compute_mrr(pred_cont[:num_pos], fake_preds)\n",
    "    \n",
    "    return avgpr_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47737458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def htlstm_test(model, test_data, data, H_1, C_1, H_2, C_2, device='cpu'):\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    num_pos = (len(test_data['user','buy','item'].edge_label_index[0])//2)\n",
    "\n",
    "    h, *_ = model(test_data.x_dict, test_data.edge_index_dict, test_data['user','buy','item'].edge_label_index,\\\n",
    "                 H_1, C_1, H_2, C_2)\n",
    "    \n",
    "    fake, *_ = model(test_data.x_dict, test_data.edge_index_dict, test_data['user','buy','item'].corrupted_edge_index,\\\n",
    "                    H_1, C_1, H_2, C_2)\n",
    "    \n",
    "    pred_cont = torch.sigmoid(h).cpu().detach().numpy()\n",
    "    fake_preds = torch.sigmoid(fake).cpu().detach().numpy()\n",
    "    \n",
    "    label = test_data['user','buy','item'].edge_label.cpu().detach().numpy()\n",
    "      \n",
    "    avgpr_score = average_precision_score(label, pred_cont)\n",
    "    mrr_score = compute_mrr(pred_cont[:num_pos], fake_preds)\n",
    "    \n",
    "    return avgpr_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_conv_1=256\n",
    "hidden_conv_2=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "device = torch.device('cuda')\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a093c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.core import magic_arguments\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "@magic_arguments.magic_arguments()\n",
    "@magic_arguments.argument('output', type=str, default='', nargs='?',\n",
    "    help=\"\"\"The name of the variable in which to store output.\n",
    "    This is a utils.io.CapturedIO object with stdout/err attributes\n",
    "    for the text of the captured output.\n",
    "    CapturedOutput also has a show() method for displaying the output,\n",
    "    and __call__ as well, so you can use that to quickly display the\n",
    "    output.\n",
    "    If unspecified, captured output is discarded.\n",
    "    \"\"\"\n",
    ")\n",
    "@magic_arguments.argument('--no-stderr', action=\"store_true\",\n",
    "    help=\"\"\"Don't capture stderr.\"\"\"\n",
    ")\n",
    "@magic_arguments.argument('--no-stdout', action=\"store_true\",\n",
    "    help=\"\"\"Don't capture stdout.\"\"\"\n",
    ")\n",
    "@magic_arguments.argument('--no-display', action=\"store_true\",\n",
    "    help=\"\"\"Don't capture IPython's rich display.\"\"\"\n",
    ")\n",
    "@register_cell_magic\n",
    "def tee(line, cell):\n",
    "    args = magic_arguments.parse_argstring(tee, line)\n",
    "    out = not args.no_stdout\n",
    "    err = not args.no_stderr\n",
    "    disp = not args.no_display\n",
    "    with capture_output(out, err, disp) as io:\n",
    "        get_ipython().run_cell(cell)\n",
    "    if args.output:\n",
    "        get_ipython().user_ns[args.output] = io\n",
    "    \n",
    "    io()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96a638",
   "metadata": {},
   "source": [
    "Due to memory issues, we strongly suggest to run the training for each candidate models separately, using the training_\"model\" functions, where model can be \\{gat, han, gconvgru, ev, hev, durendal, atu\\}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee exp_durendal\n",
    "training_gat(snapshots, hidden_conv_1, hidden_conv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_durendal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
